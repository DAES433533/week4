{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLDN CSV to Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook converts monthly CSV-formatted files of real-time lightning flashes/strikes as reported by the [National Lightning Detection Network](https://www.vaisala.com/en/products/national-lightning-detection-network-nldn) (NLDN, originally developed in our department in the 1980s!) into Parquet (a binary format ideally suited for tabular datasets, especially when hosted on cloud platforms) and then visualizes data from the Parquet file.\n",
    "### We will also compare the performance (i.e., time to execute) of reading in these files, as well as code cells in general, via the use of one of Jupyterlab's *cell magic* directives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import geopandas as gpd\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the first day of a month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = datetime(2025,8,1)\n",
    "month = current.strftime(\"%Y%m\")\n",
    "month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an object that points to the August 2025 NLDN data file in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLDN_csv = f'/spare11/atm533/data/NLDN_{month}.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a peek at the first five lines of the file, using the **!** directive to execute a Linux command as if we were typing it on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 $NLDN_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the file has several columns, each separated by one or more blank spaces (often termed *whitespace*). Since Pandas `read_csv` function's defaults to expecting commas (`,`) as the column separators, we will need to explicitly pass in \"one or more blank spaces\" as the value of the `sep` argument. Although not intuitive, that value is `'\\\\s+'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no header row at the beginning of the file, so we will also need to define a list of abbreviated column `names`; one for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ['Date', 'Time', 'Lat', 'Lon', 'Cur', 'Mul', 'CG', 'Chi', 'Maj', 'Axis', 'Min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a Jupyterlab [*cell magic* directive](https://ipython.readthedocs.io/en/9.2.0/interactive/magics.html) to determine how long it takes the cell to execute. As we will see, the file has many millions of rows, so it will take a little time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read in the CSV file\n",
    "df = pd.read_csv(f'/spare11/atm533/data/NLDN_{month}.txt',sep='\\\\s+', names = colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another library gaining traction in the Pangeo ecosystem is [Polars](https://pola.rs). Examine how its `read_csv` method compares to and contrasts with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try reading in the file with Polars. ChatGPT recommended that we state the \"one or more blank spaces\" as `r\"\\s+\", so let's see if it succeeds. **SPOILER ALERT: It will fail!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Read in the CSV file\n",
    "df = pl.read_csv(f'/nldn11/combined/archive/NLDN_{month}.txt' ,separator=r\"\\s+\", new_columns = colNames, has_header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why did it fail, and will it ever be an option? Check out this [GitHub Issue thread](https://github.com/pola-rs/polars/issues/12829) with a response from one of the main developers of Polars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For large tabular datasets, the convenience of having them represented in a human-readable format such as `csv` is outweighed by larger file sizes, slower read performance, greater system memory usage, and (as we have just seen) lack of support by Polars when columns are separated in non-standard ways. A binary format that solves these issues is called [Parquet](https://parquet.apache.org). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to Parquet format. Set the output directory and file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDir = '.' # Use the current directory\n",
    "parqFile = f'{outputDir}/NLDN_{month}_full.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of Pandas file output functions (in this case, [to_parquet](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html)) to perform the conversion and output to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.to_parquet(parqFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the Parquet file to verify all looks good. First, we'll read it in with Pandas. We'll redefine the dataframe object so we avoid multiple instances of fairly large datasets remaining in system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = pd.read_parquet(parqFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df_polars = pl.read_parquet(parqFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the load times compare? Try re-running the cells, and also try re-running in different order. Does the comparison change as a result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the representations of these two dataframes. What do you notice that's similar and different between them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's recast and combine some of the columns; particularly, those related to date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Combine the date and time columns into a single series, and then drop the original two series.\n",
    "\n",
    "dateTimeObj = df['Date'] + ' ' + df['Time']\n",
    "\n",
    "# Create a new column, now a complete DateTime series\n",
    "df['DateTime'] = pd.to_datetime(dateTimeObj,format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "# Drop the original two series, as well as some others we are not interested in. This also reduces memory usage!\n",
    "\n",
    "df = df.drop(columns=['Date','Time','Mul','Chi','Maj','Axis','Min'])\n",
    "\n",
    "# Reorder the columns\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "\n",
    "# Convert the `CG` column into booleans, following https://statisticsglobe.com/convert-string-boolean-pandas-dataframe-column-python .\n",
    "\n",
    "CGstr = df['CG'].str.strip() # Strip off any leading / trailing whitespace\n",
    "df['CG'] = CGstr.map({'G': True, 'C': False})\n",
    "\n",
    "# View the modified dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = df['DateTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specify a time range over which we wish to sample lightning events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd =  datetime(2025, 8, 15, 21)\n",
    "ed =  datetime(2025, 8, 15, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range= pd.date_range(sd, ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataframe, select only those times that are within that time range. We'll use Pandas [query](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html) method, which uses database-like syntax to quickly perform subsetting. We'll further only include cloud-to-ground strikes (not intra/inter-cloud flashes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df.query('@date_time >= @sd & @date_time <= @ed')\n",
    "CG = df['CG']\n",
    "df_subset = df.query('@CG == True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, *georeference* this dataframe, using [Geopandas](https://geopandas.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lons, lats = df_subset.Lon, df_subset.Lat\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df_subset,geometry=gpd.points_from_xy(lons,lats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order for data to properly render on an interactive map, we must assign a coordinate reference system to it. Since the coordinates represent latitude and longitude in degrees, we'll use [WGS84 lat-lon projection](https://geopandas.org/en/stable/docs/user_guide/projections.html), aka *EPSG:4326*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.set_crs(epsg=4326, inplace=True, allow_override=True)\n",
    "\n",
    "# Drop the Lat and Lon columns since they are now handled by the geometry column\n",
    "gdf = gdf.drop(columns=['Lat', 'Lon'])\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the transformed dataframe. What do you think the `geometry` column represents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note! It takes quite a while to render a large number of points on this interactive map. If your subsetted dataframe is more than 100,000 rows, you will definitely want to further restrict your time range, or you could also consider subsetting your geographical extent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's interactively visualize the lightning strikes! You can zoom in/out, pan around, and mouse over individual strikes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a fairly memory-intensive notebook. Please make sure you *close and shutdown* when done running it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Sep 2025 Environment (Pixi)",
   "language": "python",
   "name": "sep25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
